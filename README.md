# Clinical Report Query Generator

AI-powered clinical data analysis platform that combines traditional SQL-based data extraction with advanced LLM-based analysis capabilities.

## ğŸ¯ Overview

**Clinical Report Query Generator** is a Streamlit-based application that enables pharmaceutical researchers and clinical data analysts to generate SQL queries and insights from natural language requests. The system leverages Google's Gemini AI models for intelligent query generation and provides real-time execution on Databricks.

### Key Features

- **ğŸ¥ Disease Pipeline**: Disease-centric analysis with 4 core + 7 AI-recommended recipes
- **ğŸ’¬ NL2SQL**: Natural language to SQL conversion with RAG pattern and real-time execution
- **ğŸ“Š Schema Chatbot**: Interactive Q&A assistant for database schema understanding
- **ğŸ“ˆ Auto Chart Recommendation**: Smart data visualization based on result patterns
- **â­ Query History**: Persistent storage with favorites and reuse functionality

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Google Gemini API Key
- Databricks SQL Warehouse Access (optional, for query execution)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd clinical_report_generator
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure API credentials:
```bash
# Copy and edit config.yaml
cp config.yaml.example config.yaml
# Add your Gemini API key and Databricks credentials
```

### Running the Application

```bash
streamlit run app.py
```

The application will be available at `http://localhost:8501`

## ğŸ“ Project Structure

```
clinical_report_generator/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ ARCHITECTURE.md             # Detailed system architecture
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ features/                   # UI Components (3 Streamlit tabs)
â”‚   â”œâ”€â”€ disease_pipeline_tab.py
â”‚   â”œâ”€â”€ nl2sql_tab.py
â”‚   â””â”€â”€ schema_chatbot_tab.py
â”‚
â”œâ”€â”€ pipelines/                  # Business Logic Orchestration
â”‚   â”œâ”€â”€ disease_pipeline.py
â”‚   â””â”€â”€ nl2sql_generator.py
â”‚
â”œâ”€â”€ components/                 # Reusable UI Components
â”‚   â””â”€â”€ chart_builder.py
â”‚
â”œâ”€â”€ core/                       # Domain Logic
â”‚   â”œâ”€â”€ recipe_loader.py
â”‚   â””â”€â”€ sql_template_engine.py
â”‚
â”œâ”€â”€ services/                   # External APIs
â”‚   â”œâ”€â”€ gemini_service.py
â”‚   â”œâ”€â”€ databricks_client.py
â”‚   â”œâ”€â”€ schema_chatbot.py
â”‚   â””â”€â”€ parameter_extractor.py
â”‚
â”œâ”€â”€ utils/                      # Pure Utilities
â”‚   â”œâ”€â”€ parsers.py
â”‚   â”œâ”€â”€ formatters.py
â”‚   â”œâ”€â”€ visualization.py
â”‚   â”œâ”€â”€ session_state.py
â”‚   â”œâ”€â”€ query_history.py
â”‚   â””â”€â”€ chart_recommender.py
â”‚
â”œâ”€â”€ prompts/                    # LLM Prompt Templates
â”‚   â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ report_generation/
â”‚   â”œâ”€â”€ recipe_recommendation/
â”‚   â””â”€â”€ nl2sql/
â”‚
â”œâ”€â”€ recipes/                    # SQL Recipe Templates (42 recipes)
â”‚   â”œâ”€â”€ pool/                   # 10 patient pool recipes
â”‚   â””â”€â”€ profile/                # 32 patient profile recipes
â”‚
â”œâ”€â”€ tests/                      # Test Suite
â”‚   â”œâ”€â”€ unit/                   # Unit tests
â”‚   â”œâ”€â”€ integration/            # Integration tests
â”‚   â””â”€â”€ reports/                # Test result reports
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ DEVLOG.md              # Development history
â”‚   â”œâ”€â”€ CLAUDE_GUIDE.md        # AI assistant guide
â”‚   â”œâ”€â”€ DATABRICKS_SETUP.md
â”‚   â”œâ”€â”€ implementation/         # Implementation guides
â”‚   â”œâ”€â”€ archive/                # Deprecated files
â”‚   â””â”€â”€ sql_debug/              # Debug SQL queries
â”‚
â””â”€â”€ tools/                      # Development tools
    â””â”€â”€ generate_all_sql.py
```

## ğŸ”§ Configuration

### config.yaml

```yaml
api_keys:
  gemini_api_key: "YOUR_GEMINI_API_KEY"

databricks:
  server_hostname: "adb-xxx.azuredatabricks.net"
  http_path: "/sql/1.0/warehouses/xxx"
  access_token: "dapiXXXXXXXX"
```

Alternatively, use environment variables:
```bash
export DATABRICKS_SERVER_HOSTNAME="adb-xxx.azuredatabricks.net"
export DATABRICKS_HTTP_PATH="/sql/1.0/warehouses/xxx"
export DATABRICKS_TOKEN="dapiXXXXXXXX"
```

## ğŸ“– Usage

### Tab 1: Disease Pipeline

1. Enter a disease keyword (e.g., "ê³ í˜ˆì••", "ë‹¹ë‡¨ë³‘")
2. System executes 4 core recipes automatically
3. AI recommends 7 additional recipes based on disease characteristics
4. Review and select desired recipes
5. Optionally refine with natural language feedback
6. Execute approved recipes and view results

### Tab 2: NL2SQL

1. Enter natural language query (e.g., "ê³ í˜ˆì•• í™˜ìì˜ ì„±ë³„ ë¶„í¬")
2. Click "ğŸš€ SQL ìƒì„±" to generate SQL
3. Review generated SQL and quality metrics
4. Click "â–¶ï¸ ì¿¼ë¦¬ ì‹¤í–‰" to execute on Databricks
5. View results with auto-recommended charts
6. Save to favorites for reuse

### Tab 3: Schema Chatbot

1. Ask questions about database schema
2. System retrieves relevant tables/columns using RAG
3. AI provides detailed explanations and examples
4. Maintains conversation history for follow-up questions

## ğŸ¨ Features in Detail

### Auto Chart Recommendation (Phase 18)

The system automatically analyzes query results and recommends optimal chart types:
- **1 column**: Histogram or bar/pie chart
- **2 columns**: Categorical+numeric â†’ pie/bar, numeric+numeric â†’ scatter
- **3+ columns**: Bar chart with optional color grouping

**8 Chart Types**: Bar, Line, Scatter, Line+Scatter, Pie, Area, Box, Histogram

**7 Color Palettes**: Clinical, Nature, Science, Colorblind-friendly, Blue Gradient, Professional, Default

### Query History & Favorites (Phase 19)

- **Auto-save**: Every generated query is automatically saved
- **Favorites**: Star frequently used queries
- **Reuse**: One-click copy to input field
- **Statistics**: Track success rates and execution times
- **Export**: Export selected queries to SQL file

### Production Stability Features (Phase 16)

- **Safe Date Parsing**: Uses `TRY_TO_DATE()` to handle invalid dates
- **User-Friendly Errors**: Categorized error messages with troubleshooting steps
- **Comprehensive Logging**: Daily log files with query performance tracking

## ğŸ§ª Testing

### Run Unit Tests
```bash
python -m pytest tests/unit/
```

### Run Integration Tests
```bash
python -m pytest tests/integration/
```

### Run All Tests
```bash
python -m pytest tests/
```

## ğŸ“š Documentation

- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - Detailed system architecture and design decisions
- **[docs/DEVLOG.md](./docs/DEVLOG.md)** - Complete development history (Phase 1-19)
- **[docs/CLAUDE_GUIDE.md](./docs/CLAUDE_GUIDE.md)** - Comprehensive guide for AI assistants
- **[docs/DATABRICKS_SETUP.md](./docs/DATABRICKS_SETUP.md)** - Databricks configuration guide

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit
- **AI/LLM**: Google Gemini 2.5-Flash
- **Database**: Databricks (Spark SQL)
- **Visualization**: Plotly
- **Template Engine**: Jinja2
- **Data Processing**: Pandas

## ğŸ“Š System Metrics

- **42 SQL Recipes**: Pre-built templates for common analyses
- **96% Execution Success Rate**: Phase 15 test results
- **100% SQL Generation Success**: With RAG-enhanced schema understanding
- **8 Chart Types**: Professional data visualization options
- **7 Color Palettes**: Including colorblind-friendly options

## ğŸ” Security Features

- **Privacy Masking**: Automatic masking of personal data (name, phone, SSN)
- **SQL Injection Prevention**: Parameterized queries and validation
- **Access Control**: Databricks token-based authentication

## ğŸ› Known Issues & Limitations

See [docs/DEVLOG.md](./docs/DEVLOG.md) "Technical Debt" section for:
- Duplicate SQL rendering logic
- Missing automated tests for some modules
- Config management improvements needed
- Type hints consistency

## ğŸš§ Roadmap

### Planned Improvements
1. **Automated Testing**: Comprehensive pytest test suite for all layers
2. **Type Hints**: Full mypy compliance
3. **Config Management**: Unified configuration module with validation
4. **Code Consolidation**: Eliminate duplicate SQL rendering logic
5. **Monitoring Dashboard**: Visualize query success rates and performance

## ğŸ“ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

[Add license information]

## ğŸ‘¥ Authors

[Add author information]

## ğŸ™ Acknowledgments

- Google Gemini API for LLM capabilities
- Databricks for SQL Warehouse infrastructure
- Streamlit for rapid UI development

---

**Last Updated**: 2025-10-19 (Project reorganization completed)
**Version**: Phase 19 (Query History & Favorites)
